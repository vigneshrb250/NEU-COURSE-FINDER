{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff876b24-108a-4454-88c2-2a3d132f3959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8c85cb-78c2-461d-8e59-1c246f05d861",
   "metadata": {},
   "source": [
    "### Extract Subject URLs from the base URL and extract Course details from these subject URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3199f91b-7099-44d4-a49d-591a8e31db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_subject_urls(base_url):\n",
    "    response = requests.get(base_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    subject_links = soup.select('ul.nav.levelone li a')\n",
    "    url = 'https://catalog.northeastern.edu'\n",
    "    return [url + link['href'] for link in subject_links]\n",
    "\n",
    "\n",
    "def scrape_courses(subject_url):\n",
    "    response = requests.get(subject_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    courses = []\n",
    "\n",
    "    for course_block in soup.select('.courseblock'):\n",
    "        title_tag = course_block.select_one('.courseblocktitle strong')\n",
    "        title = title_tag.get_text(strip=True) if title_tag else \"N/A\"\n",
    "\n",
    "        desc_tag = course_block.select_one('p.cb_desc')\n",
    "        description = desc_tag.get_text(strip=True) if desc_tag else \"N/A\"\n",
    "\n",
    "        extra_info = []\n",
    "        for extra in course_block.select('p.courseblockextra'):\n",
    "            extra_info.append(extra.get_text(strip=True))\n",
    "\n",
    "        courses.append({\n",
    "            'title': title,\n",
    "            'description': description,\n",
    "            'extras': extra_info\n",
    "        })\n",
    "\n",
    "    return courses\n",
    "\n",
    "\n",
    "base_url = 'https://catalog.northeastern.edu/course-descriptions/'\n",
    "subject_urls = get_subject_urls(base_url)\n",
    "\n",
    "all_courses = []\n",
    "for url in subject_urls:\n",
    "    all_courses.extend(scrape_courses(url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a14a9a8-5f06-4cba-b4c1-0374b58103a0",
   "metadata": {},
   "source": [
    "### Creating Llama Index Documents from the Course list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f34a79c-0bb7-4724-9913-42b838ab30b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "def create_documents(course_data):\n",
    "    documents = []\n",
    "    for course in course_data:\n",
    "        full_text = course[\"title\"] + \"\\n\" + course[\"description\"]\n",
    "        if course.get(\"extras\"):\n",
    "            full_text += \"\\n\" + \"\\n\".join(course[\"extras\"])\n",
    "        documents.append(Document(text=full_text, metadata={\"title\": course[\"title\"]}))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "880843ff-3785-4d1f-bbf5-1fc10cd607f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = create_documents(all_courses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a000f4ac-8599-486f-b129-608aaf201614",
   "metadata": {},
   "source": [
    "### Chunking, Indexing and storing the documents in a vector database(Chromadb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d01adf1-69be-4f74-bc6b-a12c5bcb6c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vigne\\anaconda3\\envs\\neu_course_finder\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "db = chromadb.PersistentClient(path=r\"C:\\Users\\vigne\\Desktop\\Higher studies\\Northeastern University Boston\\Courses\\Semester 2\\DS5983(LLMs)\\NEU-COURSE-FINDER\\NeuCourses_Chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"NeuCourses_Chroma_db\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=100, chunk_overlap=10),\n",
    "        HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"),\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11db6991-1ba9-4b0d-bf7a-95880009ecfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 100 documents...\n",
      "Processing chunk with 2 documents...\n"
     ]
    }
   ],
   "source": [
    "def chunk_list(lst, chunk_size):\n",
    "    for i in range(0, len(lst), chunk_size):\n",
    "        yield lst[i:i + chunk_size]\n",
    "\n",
    "for chunk in chunk_list(documents, 100):  \n",
    "    print(f\"Processing chunk with {len(chunk)} documents...\")\n",
    "    await pipeline.arun(documents=chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a548a1e2-8727-46b6-8a1a-36adefdeed0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in DB: 14726\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents in DB:\", chroma_collection.count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (neu_course_finder)",
   "language": "python",
   "name": "neu_course_finder"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
